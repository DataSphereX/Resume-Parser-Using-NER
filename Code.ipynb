{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVyoOem-8UbO",
        "outputId": "7d344fd0-f0bb-46b3-d12f-d627bb4dcfcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tika) (67.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from tika) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (1.26.15)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32641 sha256=20bee8df5d56b3f08089d62f4873da9f3e3639ee05628cd28412ba70e204b12f\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/56/18/e752060632d32c39c9c4545e756dad281f8504dafcfac02b95\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tika"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvMpuZxBvCo6",
        "outputId": "c3041d44-f962-472d-f7d5-44a695c2b058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract from pdf files"
      ],
      "metadata": {
        "id": "HcOKI2DvZQeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tika import parser\n",
        "import os\n",
        "\n",
        "list_files = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.pdf'):\n",
        "    raw = parser.from_file(file)\n",
        "    list_files.append(list(raw['content']))"
      ],
      "metadata": {
        "id": "O-WNPRNIRR6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample of 1 Resume"
      ],
      "metadata": {
        "id": "eyaesXOtZWK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tika import parser\n",
        "\n",
        "raw = parser.from_file('/content/Akshay.pdf')\n",
        "\n",
        "raw.get('content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "-tq_dIE08alm",
        "outputId": "9b29cba3-a54c-4239-cda0-51fcba447ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDATA SCIENTIST\\nAKSHAY KACHROO  akshaykachroo2050@gmail.com  DOB: 28th Nov 1994\\n\\n +919711138632, +918700059792\\n\\uf0e0 \\uf0ac\\n\\n\\uf095 \\uf041 Nationality: India\\n linkedin.com/in/akshay-kachroo  kachrooakshay95\\uf0e1 \\uf09b\\n\\nEDUCATION\\nDelhi University Apr. 2013  to Mar. 2017\\nB. Tech: Electronics and Communication\\nScore: 69% First Division\\n\\nDAV Public School Mar. 2012  to Apr. 2013\\nHigh School\\nScore: 92%\\n\\nSUMMARY\\n\\nData Scientist with 5+ years of relevant experience in IT - AI ML Analytics, keenly\\ninterested in Data Analytics and Machine Learning. Well-versed in numerous\\nprogramming languages and tools with strong expertise in Python, PL/SQL, and R.\\n\\nPython, SQL, NLP, Machine Learning, Data Analysis, Dashboarding, RStudio, Qlik, GIT, Market Mix Modelling, AWS, PowerBI\\n\\nSKILLS\\n\\nEMPLOYMENT\\nACCENTURE PVT LTD Delhi, India\\nData Scientist July 2021  to Current\\n• Working as an ML Engineer on the MLOps platform, whereby designing the end-to-end CI/CD pipeline for clients.\\n• Building robust dashboards for certain models like Customer Segmentation and Market Mix Models.\\n• Using NLP to perform text analytics and summarizing the results on Qlik.\\n• Automating the Industrialized ML Pipeline on the client’s AWS platform.\\n• Engagement in designing the initial architecture and data model of the Market mix project.\\n• Using Sentiment Analysis and Text Summarization techniques to summarize the large chunks of client data.\\xa0\\n\\nAXTRIA INDIA PVT LTD Delhi, India\\nDecision Scientist July 2020  to July 2021\\n• Understanding the US Healthcare based Bigdata and through ETL analysis and dashboards, providing/reporting data stories to the clients to help them make better business decisions.\\n• Handling end-to-end data analytics pipeline to provide solutions to business problems.\\n• Using languages like Python, R, Excel to build the predictive model and perform analysis.\\n\\nTATA CONSULTANCY SERVICES Indore, India\\nSystem Engineer Mar. 2018  to July 2020\\n• Managing and visualizing the data using ETL based analytics tools like Qlik sense, SAP Hana.\\n• Handling the fixes and bugs of the Python and Java based application.\\n• Handling the server side (database, jobs, procedures, batch & application severs) of the client industry with a good exposure to shell-scripting.\\n• Engaging into Deployment and patching activity and providing the technical solutions to Clients within a pre-defined SLA.\\n\\nCE INFO SYSTEMS PVT LTD Delhi, India\\nGIS Analyst Sept. 2017  to Mar. 2018\\n• Managed the Geographical data using GIS tool, MySQL and providing navigational solutions to the client.\\n• Handling and validating various spectrum of Navigational data to enhance the user experience.\\n\\nPROJECTS\\nDONORS CHOOSE PREDICTION\\n• Implementation of predictive model using the donor’s choose dataset to predict the number of projects which will be approved in respect to the total submitted ones.\\n• Data cleaning using Imputer method and NLP stop words method to perform the data preprocessing.\\n• Data visualization performed on each feature to understand the outliers, skewness (if any).\\n• Splitting the dataset into train and test and then converting the dataset into vectors form as a core step towards feature engineering.\\n• With the help of GridsearchCV finding the best hyper parameters and then applying all the above-mentioned modules on the train and test dataset until found the best model with least\\nlog loss and highest accuracy.\\n\\nPREDICTION OF FOOD DELIVERY TIME\\n• Here I had to build an end-to-end model to predict the time taken by restaurant in delivering the post after placement of online order.\\n• The data present was imbalanced hence weights assigned to each featured were manually tuned as a parameter given to Grid search library in order to balance the dataset as a feature\\nengineering.\\n• Data preprocessing was performed on text using BOW approach and then vectorized data to undergo the multiple modeling techniques along with ensemble technique in order to boost\\nthe accuracy.\\n\\nAWARDS\\nAccenture Pvt Ltd · STAR AWARD\\n• Awarded the STAR AWARD for leading with innovation.\\n\\nAxtria India Pvt LtdBRAVO AWARD\\n• Awarded the BRAVO AWARD for lasting impact and great contribution to the team at Axtria.\\n\\nmailto:akshaykachroo2050@gmail.com\\nhttp://linkedin.com/in/akshay-kachroo\\nhttps://www.github.com/kachrooakshay95\\nhttps://github.com/kachrooakshay95\\nhttps://github.com/kachrooakshay95\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = raw.get('content')"
      ],
      "metadata": {
        "id": "GYdrlU7ahUD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertion to Lower Case"
      ],
      "metadata": {
        "id": "3OLarQptZzyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw['content'] = raw['content'].lower()"
      ],
      "metadata": {
        "id": "BXNpKePZZ25T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removal of Punctuations"
      ],
      "metadata": {
        "id": "WVgul76KaCTI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3hl3a7fSb3Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing punctuations string\n",
        "punc = '''!()-[]{};:'\"\\,<>./?#$%^&*_~'''\n",
        "\n",
        "# Removing punctuations in string\n",
        "# Using loop + punctuation string\n",
        "for ele in text:\n",
        "    if ele in punc:\n",
        "        text = text.replace(ele, \"\")\n",
        "\n",
        "text = \" \".join(text.split('\\n'))"
      ],
      "metadata": {
        "id": "bONppIjBb4bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvTd826RcnDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords Removal"
      ],
      "metadata": {
        "id": "Zj2L6z0laTc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('http')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])"
      ],
      "metadata": {
        "id": "VYEiHKiUe5O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = remove_stopwords(text)"
      ],
      "metadata": {
        "id": "mTiGuA9ke9Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "HThhNwIzf5zf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "5eb68efe-55b9-436d-a2cb-da623d5393a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DATA SCIENTIST AKSHAY KACHROO akshaykachroo2050@gmailcom DOB 28th Nov 1994 +919711138632 +918700059792 \\uf0e0 \\uf0ac \\uf095 \\uf041 Nationality India linkedincominakshaykachroo kachrooakshay95\\uf0e1 \\uf09b EDUCATION Delhi University Apr 2013 Mar 2017 B Tech Electronics Communication Score 69 First Division DAV Public School Mar 2012 Apr 2013 High School Score 92 SUMMARY Data Scientist 5+ years relevant experience IT AI ML Analytics keenly interested Data Analytics Machine Learning Wellversed numerous programming languages tools strong expertise Python PLSQL R Python SQL NLP Machine Learning Data Analysis Dashboarding RStudio Qlik GIT Market Mix Modelling AWS PowerBI SKILLS EMPLOYMENT ACCENTURE PVT LTD Delhi India Data Scientist July 2021 Current • Working ML Engineer MLOps platform whereby designing endtoend CICD pipeline clients • Building robust dashboards certain models like Customer Segmentation Market Mix Models • Using NLP perform text analytics summarizing results Qlik • Automating Industrialized ML Pipeline client’s AWS platform • Engagement designing initial architecture data model Market mix project • Using Sentiment Analysis Text Summarization techniques summarize large chunks client data AXTRIA INDIA PVT LTD Delhi India Decision Scientist July 2020 July 2021 • Understanding US Healthcare based Bigdata ETL analysis dashboards providingreporting data stories clients help make better business decisions • Handling endtoend data analytics pipeline provide solutions business problems • Using languages like Python R Excel build predictive model perform analysis TATA CONSULTANCY SERVICES Indore India System Engineer Mar 2018 July 2020 • Managing visualizing data using ETL based analytics tools like Qlik sense SAP Hana • Handling fixes bugs Python Java based application • Handling server side database jobs procedures batch application severs client industry good exposure shellscripting • Engaging Deployment patching activity providing technical solutions Clients within predefined SLA CE INFO SYSTEMS PVT LTD Delhi India GIS Analyst Sept 2017 Mar 2018 • Managed Geographical data using GIS tool MySQL providing navigational solutions client • Handling validating various spectrum Navigational data enhance user experience PROJECTS DONORS CHOOSE PREDICTION • Implementation predictive model using donor’s choose dataset predict number projects approved respect total submitted ones • Data cleaning using Imputer method NLP stop words method perform data preprocessing • Data visualization performed feature understand outliers skewness • Splitting dataset train test converting dataset vectors form core step towards feature engineering • With help GridsearchCV finding best hyper parameters applying abovementioned modules train test dataset found best model least log loss highest accuracy PREDICTION OF FOOD DELIVERY TIME • Here I build endtoend model predict time taken restaurant delivering post placement online order • The data present imbalanced hence weights assigned featured manually tuned parameter given Grid search library order balance dataset feature engineering • Data preprocessing performed text using BOW approach vectorized data undergo multiple modeling techniques along ensemble technique order boost accuracy AWARDS Accenture Pvt Ltd · STAR AWARD • Awarded STAR AWARD leading innovation Axtria India Pvt LtdBRAVO AWARD • Awarded BRAVO AWARD lasting impact great contribution team Axtria mailtoakshaykachroo2050@gmailcom httplinkedincominakshaykachroo httpswwwgithubcomkachrooakshay95 httpsgithubcomkachrooakshay95 httpsgithubcomkachrooakshay95'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion to Root Words"
      ],
      "metadata": {
        "id": "Uj8bvPxvgLbI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyyd0qycuB8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "U6Jl97F5uC7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = lemmatize_words(text)"
      ],
      "metadata": {
        "id": "CcOqrSODuE-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removal of Extra Spaces"
      ],
      "metadata": {
        "id": "RLWJlEL8u0J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = re.sub(' +', ' ', text)"
      ],
      "metadata": {
        "id": "XjX-s27juy5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "Vkr1jBDkRAlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "796aa5de-1832-4931-b2b1-b449bcf7146c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Indeed Resume Michael Smith BI Big Data Azure Manchester UK Email Indeed indeedcomrfalicent140749dace5dc26f 10+ year Experience Designing Development Administration Analysis Management inthe Business Intelligence Data warehousing Client Server Technologies Webbased Applications cloud solution Databases Data warehouse Data analysis star snow flake schema data modeling design specific todata warehousing business intelligence environment Database Experience database designing scalability backup recovery writing andoptimizing SQL code Stored Procedures creating function view trigger index Cloud platform Worked Microsoft Azure cloud service like Document DB SQL Azure StreamAnalytics Event hub Power BI Web Job Web App Power BI Azure data lake analyticsUSQL Big Data Worked Azure data lake storeanalytics big data processing Azure data factoryto schedule USQL job Designed developed end end big data solution data insight Willing relocate Anywhere WORK EXPERIENCESoftware Engineer Microsoft Manchester UK December 2015 Present 1 Microsoft Rewards Live dashboard Description Microsoft reward loyalty program reward Users browsing shopping online Microsoft Rewards member earn point searching Bing browsing Microsoft Edge making purchase Xbox Store Windows Store Microsoft Store Plus user pick bonus point taking daily quiz tour Microsoft reward website Rewards live dashboard give live picture usage worldwide market like US Canada Australia new user registration count topbottom performing reward offer order stats weekly trend user activity order new user registration PBI tile get refreshed different frequency starting 5 second 30 minute TechnologyTools used Event hub stream analytics Power BI Responsibilities Created stream analytics job process event hub data Created Power BI live dashboard show live usage traffic weekly trend card chart showtopbottom 10 offer usage metric 2 Microsoft Rewards Data Insights Description Microsoft reward loyalty program reward Users browsing shopping online Microsoft Rewards member earn point searching Bing browsing Microsoft Edge making purchase Xbox Store Windows Store Microsoft Store Plus user pick bonus point taking daily quiz tour Microsoft reward website Rewards data insight data analytics reporting platform process 20 million user daily activity redemption across different market like US Canada Australia TechnologyTools used Cosmos Microsoft bigdata platform c Xflow job monitoring Power BI Responsibilities httpswwwindeedcomrDushyantBhatt140749dace5dc26fisid=rexdownloadikw=downloadtopco=IN httpswwwindeedcomrDushyantBhatt140749dace5dc26fisid=rexdownloadikw=downloadtopco=IN Created big data script cosmos C data extractor processor reducer data transformation Power BI dashboard 3 End end tracking Tool Description This realtime Tracking tool track different business transaction like order order response functional acknowledgement invoice flowing inside ICOE It give flexibility customer track transaction appropriate error information incase failure Based resource based access control tool give flexibility end user perform different action like view transaction search based different filter criterion view download actual message payload End end tracking tool stitch business transaction like order cash flow connects different hop inside ICOE like gateway routing server Processing server It also connects different system like ICOE partner end point SAP TechnologyTools used Azure Document db Azure web job Web APP RBAC Angular JS Responsibilities Document dB stored procedure Web job process event hub data populate Document db• Web App API Stream analytics job transform data Power BI report 4 Biztrack Tracking Tool Description This realtime Tracking tool track different business transaction like order order response functional acknowledgement invoice flowing inside ICOE It give flexibility customer track transaction appropriate error information incase failure Based resource based access control tool give flexibility end user perform different action like view transaction search based different filter criterion view download actual message payload TechnologyTools used SQL server 2014 SSIS net API Angular JS Responsibilities ETL solution transform business transaction data stored Biztalk table SQL azure table stored procedure User defined function Performance tuning Web API enhancement EDUCATION The University Manchester UK 2007 SKILLS problem solving Less 1 year project lifecycle Less 1 year project manager Less 1 year technical assistance Less 1 year ADDITIONAL INFORMATION Professional Skills Excellent analytical problem solving communication knowledge transfer interpersonalskills ability interact individual level Quick learner maintains cordial relationship project manager team member andgood performer team independent job environment Positive attitude towards superior amp peer Supervised junior developer throughout project lifecycle provided technical assistance'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second Method"
      ],
      "metadata": {
        "id": "5-sQ2XYKnt0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "#import docx2txt\n",
        "import nltk"
      ],
      "metadata": {
        "id": "hhZPP5bjpynF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "def extract(file):\n",
        "    if (file[-3:] == 'pdf') or (file[-3:] == 'PDF'):\n",
        "        fileReader = PyPDF2.PdfReader(open(file,'rb'), strict = False)\n",
        "        countpage = len(fileReader.pages)\n",
        "        count = 0\n",
        "        while count < countpage:\n",
        "            pageObj = fileReader.pages[count]\n",
        "            count +=1\n",
        "            t = pageObj.extract_text()\n",
        "            text.append(t)\n",
        "    return text"
      ],
      "metadata": {
        "id": "FHsoBeaxpv7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "def extract(file):\n",
        "    if (file[-3:] == 'pdf') or (file[-3:] == 'PDF'):\n",
        "        fileReader = PyPDF2.PdfReader(open(file,'rb'), strict = False)\n",
        "    return fileReader.extract_text()"
      ],
      "metadata": {
        "id": "t0HyO5fKz2ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to read resumes from the folder one by one\n",
        "mypath='/content/' #enter your path here where you saved the resumes\n",
        "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]"
      ],
      "metadata": {
        "id": "nOCvGtC2qeKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in onlyfiles:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWgKZBkMytZS",
        "outputId": "86f70ad6-3acb-4822-9451-6ac97cfa5763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Anjali_s.pdf\n",
            "/content/Amol_Dive_s.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume = []\n",
        "for i in onlyfiles:\n",
        "    resume.append(extract(i))"
      ],
      "metadata": {
        "id": "agdx48San3HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "1SMcTCCGpxuU",
        "outputId": "e79cbe32-e582-45ee-a035-6c5b6aa396e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3 | P a g e \\n • Create Models, Apply Models and Evaluation of models. \\n• Performs Hyperparameter tuning and feature selection techniques. \\n• Create API with the Help of Flask and Deployment on AWS EC2 Services. \\n \\n\\uf0a7 Education:                                                                     \\n• B.E | Pune University | 2019     \\n• Diploma | June 2016  \\n• HSC   | Maharashtra State | May 2013 \\n• SSC   | Maharashtra State | June 2011   \\n \\n\\uf0a7 Personal information: \\n• Address: At -Hiwergoan, Tal-Sinnar, Dist-Nashik, Sinnar, 422103 \\n• Date of birth: 27/051995  \\n• Nationality: Indian \\n• Hobbies: Playing Cricket, Playing Chess, Listen Music. \\n \\n\\uf0a7 Languages:   \\n English, Hindi, Marathi. \\n \\n\\uf0a7 Declaration: \\nI hereby declare that all the information given above is true and correct to the best of my knowledge. \\n \\n                                                                                                                                      Amol Dive '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "6O1CCqebqKUg",
        "outputId": "20abf292-3f57-4710-ae91-d8be3398f21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Indeed Resume Michael Smith BI / Big Data/ Azure Manchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f 10+ years of Experience in Designing, Development, Administration, Analysis, Management inthe Business Intelligence Data warehousing, Client Server Technologies, Web-based Applications, cloud solutions and Databases. Data warehouse: Data analysis, star/ snow flake schema data modeling and design specific todata warehousing and business intelligence environment. Database: Experience in database designing, scalability, back-up and recovery, writing andoptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake analytics(U-SQL). Big Data: Worked Azure data lake store/analytics for big data processing and Azure data factoryto schedule U-SQL jobs. Designed and developed end to end big data solution for data insights. Willing to relocate: Anywhere WORK EXPERIENCESoftware Engineer Microsoft - Manchester, UK. December 2015 to Present 1. Microsoft Rewards Live dashboards: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards live dashboards gives a live picture of usage world-wide and by markets like US, Canada, Australia, new user registration count, top/bottom performing rewards offers, orders stats and weekly trends of user activities, orders and new user registrations. the PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. Technology/Tools used Event hub, stream analytics and Power BI. Responsibilities Created stream analytics jobs to process event hub data Created Power BI live dashboard to show live usage traffic, weekly trends, cards, charts to showtop/bottom 10 offers and usage metrics. 2. Microsoft Rewards Data Insights: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards data insights is data analytics and reporting platform, processes 20 million users daily activities and redemption across different markets like US, Canada, Australia. Technology/Tools used Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI. Responsibilities https://www.indeed.com/r/Dushyant-Bhatt/140749dace5dc26f?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Dushyant-Bhatt/140749dace5dc26f?isid=rex-download&ikw=download-top&co=IN Created big data scripts in cosmos C# data extractors, processors and reducers for data transformation Power BI dashboards 3. End to end tracking Tool: Description: - This is real-time Tracking tool to track different business transactions like order, order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to customers to track their transactions and appropriate error information in-case of any failure. Based on resource based access control the tool gives flexibility to end user to perform different actions like view transactions, search based on different filter criteria and view and download actual message payload. End to end tracking tool stitches all the business transaction like order to cash flow and connects different hops inside ICOE like gateway, routing server, Processing server. It also connects different systems like ICOE, partner end point and SAP. Technology/Tools used Azure Document db, Azure web job and Web APP, RBAC, Angular JS. Responsibilities Document dB stored procedures. Web job to process event hub data and populate Document db• Web App API. Stream analytics job to transform data Power BI reports 4. Biztrack Tracking Tool: Description: - This is real-time Tracking tool to track different business transactions like order, order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to customers to track their transactions and appropriate error information in-case of any failure. Based on resource based access control the tool gives flexibility to end user to perform different actions like view transactions, search based on different filter criteria and view and download actual message payload. Technology/Tools used SQL server 2014, SSIS, .net API, Angular JS. Responsibilities ETL solution to transform business transactions data stored in Biztalk tables. SQL azure tables, stored procedures, User defined functions. Performance tuning. Web API enhancements. EDUCATION The University of Manchester - UK 2007 SKILLS problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less than 1 year), technical assistance. (Less than 1 year) ADDITIONAL INFORMATION Professional Skills Excellent analytical, problem solving, communication, knowledge transfer and interpersonalskills with ability to interact with individuals at all the levels Quick learner and maintains cordial relationship with project manager and team members andgood performer both in team and independent job environments Positive attitude towards superiors &amp; peers Supervised junior developers throughout project lifecycle and provided technical assistance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}